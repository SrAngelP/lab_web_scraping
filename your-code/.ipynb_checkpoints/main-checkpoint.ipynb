{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library documentation](http://docs.python-requests.org/en/master/#the-user-guide)  \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import random\n",
    "import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url_doc = requests.get(url);\n",
    "\n",
    "soup = BeautifulSoup(url_doc.content, 'html.parser');\n",
    "#soup\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Charlie Marsh',\n",
       " 'Tom Payne',\n",
       " 'Goooler',\n",
       " 'Adrienne Walker',\n",
       " 'Alex Gaynor',\n",
       " 'Rich Harris',\n",
       " 'Matthias Fey',\n",
       " 'Mattias Wadman',\n",
       " 'Liam DeBeasi',\n",
       " 'Nico Domino',\n",
       " 'Klaus Post',\n",
       " 'Tianon Gravi',\n",
       " 'Shahed Nasser',\n",
       " 'Olivier Halligon',\n",
       " 'Romain Beaumont',\n",
       " 'Laurent Mazare',\n",
       " 'Janosh Riebesell',\n",
       " 'Filippo Valsorda',\n",
       " 'phuocng',\n",
       " 'Brad Fitzpatrick',\n",
       " 'afc163',\n",
       " 'Michael Waskom',\n",
       " 'Yos Riady',\n",
       " 'MistEO',\n",
       " 'dgtlmoon']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = soup.find_all('h1', {'class':'h3 lh-condensed'})\n",
    "test\n",
    "a_tag = [x.a.extract() for x in test]\n",
    "#a_tag = [x for x in test[0].a.extract()]\n",
    "#print(a_tag)\n",
    "#a_tag = test[0].a.extract()\n",
    "#a_tag = [x for x in a_tag]\n",
    "#a_list= list(a_tag)\n",
    "names = [(str(x)).strip('\\n ') for lista in a_tag for x in lista] # en el strip están los dos carácteres que necesitaba\n",
    "names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3crets_scanner',\n",
       " 'tinygrad',\n",
       " 'DeepLearningExamples',\n",
       " 'mmdeploy',\n",
       " 'Umi-OCR',\n",
       " 'deep-learning-for-image-processing',\n",
       " 'esm',\n",
       " 'PaddleSeg',\n",
       " 'lama-cleaner',\n",
       " 'detectron2',\n",
       " 'MHDDoS',\n",
       " 'PaddleHub',\n",
       " 'mmdetection',\n",
       " 'mmclassification',\n",
       " 'accelerate',\n",
       " 'PaddleClas',\n",
       " 'DouZero_For_HappyDouDiZhu',\n",
       " 'pytorch_geometric',\n",
       " 'openpilot',\n",
       " 'SuperGluePretrainedNetwork',\n",
       " 'mmsegmentation',\n",
       " 'stable-diffusion-webui',\n",
       " 'sentence-transformers',\n",
       " 'examples',\n",
       " 'hcaptcha-challenger']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_doc = requests.get(url);\n",
    "\n",
    "soup = BeautifulSoup(url_doc.content, 'html.parser');\n",
    "\n",
    "test = soup.find_all('h1', {'class':'h3 lh-condensed'})\n",
    "#test = soup.find_all('span', {'class':'text-normal'})\n",
    "a_tag = [x.a.extract() for x in test]\n",
    "#test[0].svg.extract()\n",
    "names = [(str(x)).strip('\\n ') for lista in a_tag for x in lista if len(x)>3]\n",
    "#names = [(str(x)).strip('\\n ') for lista in a_tag for x in lista] # en el strip están los dos carácteres que necesitaba\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg/220px-Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Disney_Oscar_1953_%28cropped%29.jpg/170px-Disney_Oscar_1953_%28cropped%29.jpg']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_doc = urlopen(url);\n",
    "\n",
    "soup = BeautifulSoup(url_doc, 'html.parser');\n",
    "\n",
    "test = soup.find_all('img', {'class':'thumbimage'});\n",
    "test\n",
    "\n",
    "links = [x['src'] for x in test]\n",
    "\n",
    "#for element in test:\n",
    "#    print (element['src'])\n",
    "\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#mw-head',\n",
       " '#searchInput',\n",
       " 'https://en.wiktionary.org/wiki/Python',\n",
       " 'https://en.wiktionary.org/wiki/python',\n",
       " '#Snakes',\n",
       " '#Computing',\n",
       " '#People',\n",
       " '#Roller_coasters',\n",
       " '#Vehicles',\n",
       " '#Weaponry',\n",
       " '#Other_uses',\n",
       " '#See_also',\n",
       " '/w/index.php?title=Python&action=edit&section=1',\n",
       " '/wiki/Pythonidae',\n",
       " '/wiki/Python_(genus)',\n",
       " '/wiki/Python_(mythology)',\n",
       " '/w/index.php?title=Python&action=edit&section=2',\n",
       " '/wiki/Python_(programming_language)',\n",
       " '/wiki/CMU_Common_Lisp',\n",
       " '/wiki/PERQ#PERQ_3',\n",
       " '/w/index.php?title=Python&action=edit&section=3',\n",
       " '/wiki/Python_of_Aenus',\n",
       " '/wiki/Python_(painter)',\n",
       " '/wiki/Python_of_Byzantium',\n",
       " '/wiki/Python_of_Catana',\n",
       " '/wiki/Python_Anghelo',\n",
       " '/w/index.php?title=Python&action=edit&section=4',\n",
       " '/wiki/Python_(Efteling)',\n",
       " '/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " '/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " '/w/index.php?title=Python&action=edit&section=5',\n",
       " '/wiki/Python_(automobile_maker)',\n",
       " '/wiki/Python_(Ford_prototype)',\n",
       " '/w/index.php?title=Python&action=edit&section=6',\n",
       " '/wiki/Python_(missile)',\n",
       " '/wiki/Python_(nuclear_primary)',\n",
       " '/wiki/Colt_Python',\n",
       " '/w/index.php?title=Python&action=edit&section=7',\n",
       " '/wiki/Python_(codename)',\n",
       " '/wiki/Python_(film)',\n",
       " '/wiki/Monty_Python',\n",
       " '/wiki/Python_(Monty)_Pictures',\n",
       " '/wiki/Timon_of_Phlius',\n",
       " '/w/index.php?title=Python&action=edit&section=8',\n",
       " '/wiki/Cython',\n",
       " '/wiki/Pyton',\n",
       " '/wiki/Pithon',\n",
       " '/wiki/File:Disambig_gray.svg',\n",
       " '/wiki/Help:Disambiguation',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&oldid=1116589969',\n",
       " '/wiki/Help:Category',\n",
       " '/wiki/Category:Disambiguation_pages',\n",
       " '/wiki/Category:Human_name_disambiguation_pages',\n",
       " '/wiki/Category:Disambiguation_pages_with_given-name-holder_lists',\n",
       " '/wiki/Category:Disambiguation_pages_with_short_descriptions',\n",
       " '/wiki/Category:Short_description_is_different_from_Wikidata',\n",
       " '/wiki/Category:All_article_disambiguation_pages',\n",
       " '/wiki/Category:All_disambiguation_pages',\n",
       " '/wiki/Category:Animal_common_name_disambiguation_pages',\n",
       " '/wiki/Special:MyTalk',\n",
       " '/wiki/Special:MyContributions',\n",
       " '/w/index.php?title=Special:CreateAccount&returnto=Python',\n",
       " '/w/index.php?title=Special:UserLogin&returnto=Python',\n",
       " '/wiki/Python',\n",
       " '/wiki/Talk:Python',\n",
       " '/wiki/Python',\n",
       " '/w/index.php?title=Python&action=edit',\n",
       " '/w/index.php?title=Python&action=history',\n",
       " '/wiki/Main_Page',\n",
       " '/wiki/Wikipedia:Contents',\n",
       " '/wiki/Portal:Current_events',\n",
       " '/wiki/Special:Random',\n",
       " '/wiki/Wikipedia:About',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " 'https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en',\n",
       " '/wiki/Help:Contents',\n",
       " '/wiki/Help:Introduction',\n",
       " '/wiki/Wikipedia:Community_portal',\n",
       " '/wiki/Special:RecentChanges',\n",
       " '/wiki/Wikipedia:File_Upload_Wizard',\n",
       " '/wiki/Special:WhatLinksHere/Python',\n",
       " '/wiki/Special:RecentChangesLinked/Python',\n",
       " '/wiki/Wikipedia:File_Upload_Wizard',\n",
       " '/wiki/Special:SpecialPages',\n",
       " '/w/index.php?title=Python&oldid=1116589969',\n",
       " '/w/index.php?title=Python&action=info',\n",
       " '/w/index.php?title=Special:CiteThisPage&page=Python&id=1116589969&wpFormIdentifier=titleform',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452',\n",
       " '/w/index.php?title=Special:DownloadAsPdf&page=Python&action=show-download-screen',\n",
       " '/w/index.php?title=Python&printable=yes',\n",
       " 'https://commons.wikimedia.org/wiki/Category:Python',\n",
       " 'https://af.wikipedia.org/wiki/Python',\n",
       " 'https://als.wikipedia.org/wiki/Python',\n",
       " 'https://ar.wikipedia.org/wiki/%D8%A8%D8%A7%D9%8A%D8%AB%D9%88%D9%86_(%D8%AA%D9%88%D8%B6%D9%8A%D8%AD)',\n",
       " 'https://az.wikipedia.org/wiki/Python',\n",
       " 'https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)',\n",
       " 'https://be.wikipedia.org/wiki/Python',\n",
       " 'https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)',\n",
       " 'https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)',\n",
       " 'https://da.wikipedia.org/wiki/Python',\n",
       " 'https://de.wikipedia.org/wiki/Python',\n",
       " 'https://eo.wikipedia.org/wiki/Pitono_(apartigilo)',\n",
       " 'https://eu.wikipedia.org/wiki/Python_(argipena)',\n",
       " 'https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86',\n",
       " 'https://fr.wikipedia.org/wiki/Python',\n",
       " 'https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0',\n",
       " 'https://hr.wikipedia.org/wiki/Python_(razdvojba)',\n",
       " 'https://io.wikipedia.org/wiki/Pitono',\n",
       " 'https://id.wikipedia.org/wiki/Python',\n",
       " 'https://ia.wikipedia.org/wiki/Python_(disambiguation)',\n",
       " 'https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)',\n",
       " 'https://it.wikipedia.org/wiki/Python_(disambigua)',\n",
       " 'https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F',\n",
       " 'https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)',\n",
       " 'https://kg.wikipedia.org/wiki/Mboma_(nyoka)',\n",
       " 'https://la.wikipedia.org/wiki/Python_(discretiva)',\n",
       " 'https://lb.wikipedia.org/wiki/Python',\n",
       " 'https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)',\n",
       " 'https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)',\n",
       " 'https://nl.wikipedia.org/wiki/Python',\n",
       " 'https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3',\n",
       " 'https://no.wikipedia.org/wiki/Pyton',\n",
       " 'https://pl.wikipedia.org/wiki/Pyton',\n",
       " 'https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)',\n",
       " 'https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)',\n",
       " 'https://sk.wikipedia.org/wiki/Python',\n",
       " 'https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)',\n",
       " 'https://sh.wikipedia.org/wiki/Python',\n",
       " 'https://fi.wikipedia.org/wiki/Python',\n",
       " 'https://sv.wikipedia.org/wiki/Pyton',\n",
       " 'https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99',\n",
       " 'https://tr.wikipedia.org/wiki/Python_(anlam_ayr%C4%B1m%C4%B1)',\n",
       " 'https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD',\n",
       " 'https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86',\n",
       " 'https://vi.wikipedia.org/wiki/Python',\n",
       " 'https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License',\n",
       " '//foundation.wikimedia.org/wiki/Terms_of_Use',\n",
       " '//foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " '//www.wikimediafoundation.org/',\n",
       " 'https://foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " '/wiki/Wikipedia:About',\n",
       " '/wiki/Wikipedia:General_disclaimer',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " '//en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile',\n",
       " 'https://developer.wikimedia.org',\n",
       " 'https://stats.wikimedia.org/#/en.wikipedia.org',\n",
       " 'https://foundation.wikimedia.org/wiki/Cookie_statement',\n",
       " 'https://wikimediafoundation.org/',\n",
       " 'https://www.mediawiki.org/']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_doc = requests.get(url);\n",
    "\n",
    "soup = BeautifulSoup(url_doc.content, 'html.parser');\n",
    "\n",
    "#test = soup.find_all('a', {'href':True})\n",
    "#test\n",
    "links = [x['href'] for x in soup.find_all('a', {'href':True}) if len(x)>=1]\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_doc = requests.get(url);\n",
    "\n",
    "soup = BeautifulSoup(url_doc.content, 'html.parser');\n",
    "test = soup.find_all('div', {'class':'usctitlechanged'})\n",
    "titles = [x for x in test]\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'YULAN ADONAY ARCHAGA CARIAS',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'OMAR ALEXANDER CARDENAS',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'MICHAEL JAMES PRATT',\n",
       " 'RUJA IGNATOVA',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'RAFAEL CARO-QUINTERO']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_doc = requests.get(url);\n",
    "\n",
    "soup = BeautifulSoup(url_doc.content, 'html.parser');\n",
    "test = soup.find_all('h3', {'class':'title'})\n",
    "names = [(x.a.extract()).string for x in test]\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'F',\n",
       " 'earthquake2022-11-02\\xa0\\xa0\\xa023:37:30.512min ago',\n",
       " '14.52\\xa0',\n",
       " 'N\\xa0\\xa0',\n",
       " '91.71\\xa0',\n",
       " 'W\\xa0\\xa0',\n",
       " '115',\n",
       " 'M ',\n",
       " '4.9',\n",
       " '\\xa0GUATEMALA',\n",
       " '2022-11-02 23:49']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_doc = requests.get(url);\n",
    "\n",
    "soup = BeautifulSoup(url_doc.content, 'html.parser');\n",
    "test = soup.find_all('tbody',{'id': 'tbody'})\n",
    "table1 = [y for lista in [x.find_all('tr') for x in test] for y in lista]\n",
    "#td_tag = test.find_all('td',{'class':'tabev0'})\n",
    "#table2  = [x.find('td',{'class':'tabev0'}).extract() for x in table1]\n",
    "table2 = [re.split('\\s', x.get_text()) for x in table1]\n",
    "#table2[0] = [i for i in table2[0] if i]\n",
    "for row in range(len(table2)):\n",
    "    table2[row] = [x for x in table2[row] if x]\n",
    "\n",
    "#table1[0]    \n",
    "#prueba = table1[0].get_text()\n",
    "#prueba\n",
    "text = [i.text for i in table1[0] if i.text]\n",
    "text\n",
    "#prueba = table1.get_text()\n",
    "#prueba\n",
    "#len(table2[0])\n",
    "#df = pd.DataFrame(table2)\n",
    "#df\n",
    "#for row in range(len(table1)):\n",
    "#    print(row)\n",
    "#    table1[row].find_all('td',{'class':'tabev0'}).extract()\n",
    "        \n",
    "#table1[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(table1[0].td.extract())\n",
    "len(table1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "#url ='https://hackevents.co/hackathons' \n",
    "url ='https://mlh.io/seasons/2023/events'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Dates</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do-Re-Mi Hacks 3</td>\n",
       "      <td>Nov 11th - 13th</td>\n",
       "      <td>APAC Timezone,</td>\n",
       "      <td>Online</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hack2Educate</td>\n",
       "      <td>Nov 11th - 13th</td>\n",
       "      <td>Everywhere,</td>\n",
       "      <td>Worldwide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hack K-State</td>\n",
       "      <td>Nov 11th - 13th</td>\n",
       "      <td>Manhattan,</td>\n",
       "      <td>Kansas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HackUMass</td>\n",
       "      <td>Nov 11th - 13th</td>\n",
       "      <td>Amherst,</td>\n",
       "      <td>Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HarvestHacks</td>\n",
       "      <td>Nov 11th - 13th</td>\n",
       "      <td>North American Timezone,</td>\n",
       "      <td>Online</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>REVA Hack 2022</td>\n",
       "      <td>Nov 11th - 13th</td>\n",
       "      <td>Bengaluru,</td>\n",
       "      <td>Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VTHacks X</td>\n",
       "      <td>Nov 11th - 13th</td>\n",
       "      <td>Blacksburg,</td>\n",
       "      <td>Virginia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BostonHacks 2022</td>\n",
       "      <td>Nov 12th - 13th</td>\n",
       "      <td>Boston,</td>\n",
       "      <td>Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Concode</td>\n",
       "      <td>Nov 12th - 13th</td>\n",
       "      <td>Everywhere,</td>\n",
       "      <td>Worldwide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Datathon FME</td>\n",
       "      <td>Nov 12th - 13th</td>\n",
       "      <td>Barcelona,</td>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GreatUniHack</td>\n",
       "      <td>Nov 12th - 13th</td>\n",
       "      <td>Manchester,</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HackDown 2022</td>\n",
       "      <td>Nov 12th - 13th</td>\n",
       "      <td>Ewing,</td>\n",
       "      <td>New Jersey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HackHolyoke 2022</td>\n",
       "      <td>Nov 12th - 13th</td>\n",
       "      <td>Everywhere,</td>\n",
       "      <td>Worldwide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HackUTD IX</td>\n",
       "      <td>Nov 12th - 13th</td>\n",
       "      <td>Dallas,</td>\n",
       "      <td>Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>OxfordHack</td>\n",
       "      <td>Nov 12th - 13th</td>\n",
       "      <td>Oxford,</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>WHACK</td>\n",
       "      <td>Nov 12th - 13th</td>\n",
       "      <td>Wellesley,</td>\n",
       "      <td>Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hackcoming 2</td>\n",
       "      <td>Nov 18th - 20th</td>\n",
       "      <td>North American Timezone,</td>\n",
       "      <td>Online</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hacks and Crafts 2</td>\n",
       "      <td>Nov 18th - 20th</td>\n",
       "      <td>APAC Timezone,</td>\n",
       "      <td>Online</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Hack Western 9</td>\n",
       "      <td>Nov 18th - 20th</td>\n",
       "      <td>London,</td>\n",
       "      <td>Ontario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>DurHack 2022.2</td>\n",
       "      <td>Nov 19th - 20th</td>\n",
       "      <td>Durham,</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Hack Brunel</td>\n",
       "      <td>Nov 19th - 20th</td>\n",
       "      <td>London,</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MetroHacks</td>\n",
       "      <td>Nov 19th - 20th</td>\n",
       "      <td>Everywhere,</td>\n",
       "      <td>Worldwide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Reconnect-Ed</td>\n",
       "      <td>Nov 19th - 20th</td>\n",
       "      <td>Global,</td>\n",
       "      <td>Everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>UniHack</td>\n",
       "      <td>Nov 24th - 27th</td>\n",
       "      <td>Timișoara,</td>\n",
       "      <td>Romania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>CentuRITon</td>\n",
       "      <td>Nov 25th - 27th</td>\n",
       "      <td>Bengaluru,</td>\n",
       "      <td>Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Give Back Hacks 3</td>\n",
       "      <td>Nov 25th - 27th</td>\n",
       "      <td>North American Timezone,</td>\n",
       "      <td>Online</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Thehacktrical 2</td>\n",
       "      <td>Nov 25th - 27th</td>\n",
       "      <td>APAC Timezone,</td>\n",
       "      <td>Online</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Global Hack Week December</td>\n",
       "      <td>Dec 4th - 11th</td>\n",
       "      <td>Everywhere,</td>\n",
       "      <td>Worldwide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>HackFRee</td>\n",
       "      <td>Jan 14th - 15th</td>\n",
       "      <td>Manalapan,</td>\n",
       "      <td>New Jersey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ConUHacks VII</td>\n",
       "      <td>Jan 21st - 22nd</td>\n",
       "      <td>Montreal,</td>\n",
       "      <td>Quebec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Hoya Hacks</td>\n",
       "      <td>Jan 27th - 29th</td>\n",
       "      <td>Washington,</td>\n",
       "      <td>DC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Rice Datathon</td>\n",
       "      <td>Jan 27th - 29th</td>\n",
       "      <td>Houston ,</td>\n",
       "      <td>Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>TAMUhack</td>\n",
       "      <td>Jan 28th - 29th</td>\n",
       "      <td>College Station,</td>\n",
       "      <td>Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Hack This Fall</td>\n",
       "      <td>Feb 3rd - 5th</td>\n",
       "      <td>IN,</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>UGAHacks 8</td>\n",
       "      <td>Feb 3rd - 5th</td>\n",
       "      <td>Athens,</td>\n",
       "      <td>Georgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Hacklytics 2023</td>\n",
       "      <td>Feb 10th - 12th</td>\n",
       "      <td>Atlanta,</td>\n",
       "      <td>Georgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>hackTAMS</td>\n",
       "      <td>Feb 18th - 19th</td>\n",
       "      <td>Denton,</td>\n",
       "      <td>Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>AthenaHacks</td>\n",
       "      <td>Feb 25th - 26th</td>\n",
       "      <td>Los Angelos,</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Title            Dates                      City  \\\n",
       "0            Do-Re-Mi Hacks 3  Nov 11th - 13th            APAC Timezone,   \n",
       "1                Hack2Educate  Nov 11th - 13th               Everywhere,   \n",
       "2                Hack K-State  Nov 11th - 13th                Manhattan,   \n",
       "3                   HackUMass  Nov 11th - 13th                  Amherst,   \n",
       "4                HarvestHacks  Nov 11th - 13th  North American Timezone,   \n",
       "5              REVA Hack 2022  Nov 11th - 13th                Bengaluru,   \n",
       "6                   VTHacks X  Nov 11th - 13th               Blacksburg,   \n",
       "7            BostonHacks 2022  Nov 12th - 13th                   Boston,   \n",
       "8                     Concode  Nov 12th - 13th               Everywhere,   \n",
       "9                Datathon FME  Nov 12th - 13th                Barcelona,   \n",
       "10               GreatUniHack  Nov 12th - 13th               Manchester,   \n",
       "11              HackDown 2022  Nov 12th - 13th                    Ewing,   \n",
       "12           HackHolyoke 2022  Nov 12th - 13th               Everywhere,   \n",
       "13                 HackUTD IX  Nov 12th - 13th                   Dallas,   \n",
       "14                 OxfordHack  Nov 12th - 13th                   Oxford,   \n",
       "15                      WHACK  Nov 12th - 13th                Wellesley,   \n",
       "16               Hackcoming 2  Nov 18th - 20th  North American Timezone,   \n",
       "17         Hacks and Crafts 2  Nov 18th - 20th            APAC Timezone,   \n",
       "18             Hack Western 9  Nov 18th - 20th                   London,   \n",
       "19             DurHack 2022.2  Nov 19th - 20th                   Durham,   \n",
       "20                Hack Brunel  Nov 19th - 20th                   London,   \n",
       "21                 MetroHacks  Nov 19th - 20th               Everywhere,   \n",
       "22               Reconnect-Ed  Nov 19th - 20th                   Global,   \n",
       "23                    UniHack  Nov 24th - 27th                Timișoara,   \n",
       "24                 CentuRITon  Nov 25th - 27th                Bengaluru,   \n",
       "25          Give Back Hacks 3  Nov 25th - 27th  North American Timezone,   \n",
       "26            Thehacktrical 2  Nov 25th - 27th            APAC Timezone,   \n",
       "27  Global Hack Week December   Dec 4th - 11th               Everywhere,   \n",
       "28                   HackFRee  Jan 14th - 15th                Manalapan,   \n",
       "29              ConUHacks VII  Jan 21st - 22nd                 Montreal,   \n",
       "30                 Hoya Hacks  Jan 27th - 29th               Washington,   \n",
       "31              Rice Datathon  Jan 27th - 29th                 Houston ,   \n",
       "32                   TAMUhack  Jan 28th - 29th          College Station,   \n",
       "33             Hack This Fall    Feb 3rd - 5th                       IN,   \n",
       "34                 UGAHacks 8    Feb 3rd - 5th                   Athens,   \n",
       "35            Hacklytics 2023  Feb 10th - 12th                  Atlanta,   \n",
       "36                   hackTAMS  Feb 18th - 19th                   Denton,   \n",
       "37                AthenaHacks  Feb 25th - 26th              Los Angelos,   \n",
       "\n",
       "           Country  \n",
       "0           Online  \n",
       "1        Worldwide  \n",
       "2           Kansas  \n",
       "3    Massachusetts  \n",
       "4           Online  \n",
       "5        Karnataka  \n",
       "6         Virginia  \n",
       "7    Massachusetts  \n",
       "8        Worldwide  \n",
       "9            Spain  \n",
       "10  United Kingdom  \n",
       "11      New Jersey  \n",
       "12       Worldwide  \n",
       "13           Texas  \n",
       "14  United Kingdom  \n",
       "15   Massachusetts  \n",
       "16          Online  \n",
       "17          Online  \n",
       "18         Ontario  \n",
       "19  United Kingdom  \n",
       "20  United Kingdom  \n",
       "21       Worldwide  \n",
       "22      Everywhere  \n",
       "23         Romania  \n",
       "24       Karnataka  \n",
       "25          Online  \n",
       "26          Online  \n",
       "27       Worldwide  \n",
       "28      New Jersey  \n",
       "29          Quebec  \n",
       "30              DC  \n",
       "31           Texas  \n",
       "32           Texas  \n",
       "33           India  \n",
       "34         Georgia  \n",
       "35         Georgia  \n",
       "36           Texas  \n",
       "37      California  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_doc = requests.get(url);\n",
    "\n",
    "soup = BeautifulSoup(url_doc.content, 'html.parser');\n",
    "test = soup.find_all('div',{'class':'row'})\n",
    "text = test[1]\n",
    "titles = [x.get_text() for x in text.find_all('h3', {'class':'event-name'})]\n",
    "titles;\n",
    "dates = [(x.get_text()).strip('\\n ') for x in text.find_all('p', {'class':'event-date'})]\n",
    "dates;\n",
    "location = [(x.get_text()).split('\\n ') for x in text.find_all('div', {'class':'event-location'})]\n",
    "city = [(x[0]).strip('\\n ') for x in location]\n",
    "country = [(x[1]).strip('\\n ') for x in location]\n",
    "\n",
    "events = pd.DataFrame(zip(titles,dates,city,country), columns= ['Title', 'Dates', 'City', 'Country'])\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import re \n",
    "import sys\n",
    "import requests \n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User doesn't exist\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#name = 'pasta_roja';\n",
    "name = 'kaseo_real';\n",
    "link = url+name\n",
    "\n",
    "try:\n",
    "    \n",
    "    driver = webdriver.Chrome('chromedriver')\n",
    "    driver.get(link)\n",
    "    time.sleep(5)\n",
    "    pagweb = driver.page_source;\n",
    "    driver.quit();\n",
    "    product = BeautifulSoup(pagweb, 'html.parser');\n",
    "    #print(product)\n",
    "\n",
    "    twitts = product.select(\"div [class = 'css-1dbjc4n r-1habvwh'] > div[dir='auto']\")[0].text\n",
    "    print(twitts)\n",
    "except IndexError:\n",
    "    print(\"User doesn't exist\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pagweb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'207,2 mil'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#div.r-13awgt0:nth-child(5) > div:nth-child(2) > a:nth-child(1) > span:nth-child(1)\n",
    "\n",
    "#from lxml import etree\n",
    "name = 'kaseo_real';\n",
    "link = url+name\n",
    "\n",
    "try:\n",
    "    driver = webdriver.Chrome('chromedriver')\n",
    "    driver.get(link)\n",
    "    time.sleep(6)\n",
    "    pagweb = driver.page_source;\n",
    "    driver.quit();\n",
    "    product = BeautifulSoup(pagweb, 'html.parser');\n",
    "    #print(product)\n",
    "\n",
    "    #dom = etree.HTML(str(product))\n",
    "    #print(dom.xpath(xpath_address))\n",
    "\n",
    "    product_review = re.sub('\\s',' ', product.select(\"div.r-13awgt0:nth-child(5) > div:nth-child(2) > a:nth-child(1) > span:nth-child(1)\")[0].text )\n",
    "    #product_review = product.select(\"div [class = 'css-1dbjc4n'] > div[dir='auto']\")#[0].text\n",
    "    print(product_review);\n",
    "except IndexError:\n",
    "    print(\"User doesn't exist\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  English 6 458 000+ articles  ',\n",
       " '  日本語 1 314 000+ 記事  ',\n",
       " '  Español 1 755 000+ artículos  ',\n",
       " '  Русский 1 798 000+ статей  ',\n",
       " '  Français 2 400 000+ articles  ',\n",
       " '  Deutsch 2 667 000+ Artikel  ',\n",
       " '  Italiano 1 742 000+ voci  ',\n",
       " '  中文 1 256 000+ 条目 / 條目  ',\n",
       " '  Português 1 085 000+ artigos  ',\n",
       " '  العربية 1 159 000+ مقالة  ']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_doc = requests.get(url);\n",
    "\n",
    "soup = BeautifulSoup(url_doc.content, 'html.parser');\n",
    "\n",
    "test = soup.find_all('div', {'class':'central-featured-lang'})\n",
    "#test = soup.find_all('strong')\n",
    "languages = [re.sub('\\s', ' ', x.get_text()) for x in test]\n",
    "languages\n",
    "#languages = [x.get_text() for x in test[1:11]]\n",
    "#languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_doc = requests.get(url);\n",
    "\n",
    "soup = BeautifulSoup(url_doc.content, 'html.parser');\n",
    "\n",
    "test = soup.find_all('h3', {'class':'govuk-heading-s dgu-topics__heading'})\n",
    "datasets = [x.get_text() for x in test]\n",
    "datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Native speakers(millions)</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin Chinese(incl. Standard Chinese, but e...</td>\n",
       "      <td>920</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>475</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>373</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi(excl. Urdu)</td>\n",
       "      <td>344</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>234</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>232</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Russian</td>\n",
       "      <td>154</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Balto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>125</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Yue Chinese(incl. Cantonese)</td>\n",
       "      <td>85.2</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>84.6</td>\n",
       "      <td>Austroasiatic</td>\n",
       "      <td>Vietic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Language  \\\n",
       "0  Mandarin Chinese(incl. Standard Chinese, but e...   \n",
       "1                                            Spanish   \n",
       "2                                            English   \n",
       "3                                  Hindi(excl. Urdu)   \n",
       "4                                            Bengali   \n",
       "5                                         Portuguese   \n",
       "6                                            Russian   \n",
       "7                                           Japanese   \n",
       "8                       Yue Chinese(incl. Cantonese)   \n",
       "9                                         Vietnamese   \n",
       "\n",
       "  Native speakers(millions) Language family        Branch  \n",
       "0                       920    Sino-Tibetan       Sinitic  \n",
       "1                       475   Indo-European       Romance  \n",
       "2                       373   Indo-European      Germanic  \n",
       "3                       344   Indo-European    Indo-Aryan  \n",
       "4                       234   Indo-European    Indo-Aryan  \n",
       "5                       232   Indo-European       Romance  \n",
       "6                       154   Indo-European  Balto-Slavic  \n",
       "7                       125         Japonic      Japanese  \n",
       "8                      85.2    Sino-Tibetan       Sinitic  \n",
       "9                      84.6   Austroasiatic        Vietic  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_doc = requests.get(url);\n",
    "\n",
    "soup = BeautifulSoup(url_doc.content, 'html.parser');\n",
    "#test = soup.find_all('tbody')\n",
    "test = soup.find_all('table', {'class':'wikitable sortable static-row-numbers'})\n",
    "rows = test[0].find_all('tr')\n",
    "#rows = [x.get_text() for x in rows]\n",
    "rows = [list(filter(None, re.split('\\n\\n*', x.get_text()))) for x in rows if x]\n",
    "#rows = [re.sub('\\s','',x.get_text()) for row in rows for x in row i]\n",
    "#for element in range(len(rows)):\n",
    "#    rows[element] = [x for x in rows[element] if x]\n",
    "    \n",
    "df = pd.DataFrame(rows[1:], columns = rows[0])\n",
    "df[:10]\n",
    "#datasets = [x.get_text() for x in test]\n",
    "#datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ankor Inclán Oficial@ankorinclan·5 oct.Hay días que solo funcionas escuchando @KaseO_real energía para el día en vena. No consumir si no estás dispuest@ abrir tu mente.21586\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#name = 'pasta_roja';\n",
    "name = 'kaseo_real';\n",
    "link = url+name\n",
    "\n",
    "try:\n",
    "    \n",
    "    driver = webdriver.Chrome('chromedriver')\n",
    "    driver.get(link)\n",
    "    time.sleep(15)\n",
    "    ActionChains(driver).send_keys(Keys.END).perform();\n",
    "    time.sleep(3)\n",
    "    ActionChains(driver).send_keys(Keys.END).perform();\n",
    "    time.sleep(5)\n",
    "    ActionChains(driver).send_keys(Keys.END).perform();\n",
    "    time.sleep(3)\n",
    "    pagweb = driver.page_source;\n",
    "    driver.quit();\n",
    "    product = BeautifulSoup(pagweb, 'html.parser');\n",
    "    #print(product)\n",
    "\n",
    "    twitts = product.select('section.css-1dbjc4n > div:nth-child(2) > div:nth-child(1) > div:nth-child(2) > div:nth-child(1) > div:nth-child(1) > div:nth-child(1) > article:nth-child(1) > div:nth-child(1) > div:nth-child(1) > div:nth-child(1) > div:nth-child(2) > div:nth-child(2)')[0].text\n",
    "    print(twitts)\n",
    "except IndexError:\n",
    "    print(\"User doesn't exist\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Relase</th>\n",
       "      <th>Director</th>\n",
       "      <th>Actors/Actresses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.</th>\n",
       "      <td>Sueño de fuga</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>Frank Darabont (dir.)</td>\n",
       "      <td>Tim Robbins, Morgan Freeman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>(1972)</td>\n",
       "      <td>Francis Ford Coppola (dir.)</td>\n",
       "      <td>Marlon Brando, Al Pacino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.</th>\n",
       "      <td>Batman: El caballero de la noche</td>\n",
       "      <td>(2008)</td>\n",
       "      <td>Christopher Nolan (dir.)</td>\n",
       "      <td>Christian Bale, Heath Ledger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.</th>\n",
       "      <td>El padrino II</td>\n",
       "      <td>(1974)</td>\n",
       "      <td>Francis Ford Coppola (dir.)</td>\n",
       "      <td>Al Pacino, Robert De Niro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.</th>\n",
       "      <td>12 hombres en pugna</td>\n",
       "      <td>(1957)</td>\n",
       "      <td>Sidney Lumet (dir.)</td>\n",
       "      <td>Henry Fonda, Lee J. Cobb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246.</th>\n",
       "      <td>Dersu Uzala</td>\n",
       "      <td>(1975)</td>\n",
       "      <td>Akira Kurosawa (dir.)</td>\n",
       "      <td>Maksim Munzuk, Yuriy Solomin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247.</th>\n",
       "      <td>Aladdín</td>\n",
       "      <td>(1992)</td>\n",
       "      <td>Ron Clements (dir.)</td>\n",
       "      <td>Scott Weinger, Robin Williams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248.</th>\n",
       "      <td>Historias cruzadas</td>\n",
       "      <td>(2011)</td>\n",
       "      <td>Tate Taylor (dir.)</td>\n",
       "      <td>Viola Davis, Emma Stone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249.</th>\n",
       "      <td>Gandhi</td>\n",
       "      <td>(1982)</td>\n",
       "      <td>Richard Attenborough (dir.)</td>\n",
       "      <td>Ben Kingsley, John Gielgud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250.</th>\n",
       "      <td>El gigante de hierro</td>\n",
       "      <td>(1999)</td>\n",
       "      <td>Brad Bird (dir.)</td>\n",
       "      <td>Eli Marienthal, Harry Connick Jr.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Name  Relase                     Director  \\\n",
       "1.                       Sueño de fuga  (1994)        Frank Darabont (dir.)   \n",
       "2.                          El padrino  (1972)  Francis Ford Coppola (dir.)   \n",
       "3.    Batman: El caballero de la noche  (2008)     Christopher Nolan (dir.)   \n",
       "4.                       El padrino II  (1974)  Francis Ford Coppola (dir.)   \n",
       "5.                 12 hombres en pugna  (1957)          Sidney Lumet (dir.)   \n",
       "...                                ...     ...                          ...   \n",
       "246.                       Dersu Uzala  (1975)        Akira Kurosawa (dir.)   \n",
       "247.                           Aladdín  (1992)          Ron Clements (dir.)   \n",
       "248.                Historias cruzadas  (2011)           Tate Taylor (dir.)   \n",
       "249.                            Gandhi  (1982)  Richard Attenborough (dir.)   \n",
       "250.              El gigante de hierro  (1999)             Brad Bird (dir.)   \n",
       "\n",
       "                        Actors/Actresses  \n",
       "1.           Tim Robbins, Morgan Freeman  \n",
       "2.              Marlon Brando, Al Pacino  \n",
       "3.          Christian Bale, Heath Ledger  \n",
       "4.             Al Pacino, Robert De Niro  \n",
       "5.              Henry Fonda, Lee J. Cobb  \n",
       "...                                  ...  \n",
       "246.        Maksim Munzuk, Yuriy Solomin  \n",
       "247.       Scott Weinger, Robin Williams  \n",
       "248.             Viola Davis, Emma Stone  \n",
       "249.          Ben Kingsley, John Gielgud  \n",
       "250.   Eli Marienthal, Harry Connick Jr.  \n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "url_doc = requests.get(url);\n",
    "\n",
    "soup = BeautifulSoup(url_doc.content, 'html.parser');\n",
    "test = soup.select('td.titleColumn')\n",
    "info1 = [list(filter(None, re.split('\\n *', x.get_text()))) for x in test]   # Para obtener el nombre y el año, el 'filter' es para filtrar los elementos vacíos\n",
    "info2 = [re.split(',', (x.a.extract())['title'], maxsplit=1) for x in test]      #Para obtener los demás datos\n",
    "ranking = [x[0] for x in info1]\n",
    "info1 = [x[1:] for x in info1]\n",
    "info_total = list(map(operator.add, info1, info2))\n",
    "\n",
    "\n",
    "df_movies = pd.DataFrame(info_total, columns=['Name','Relase','Director','Actors/Actresses'], index=ranking)\n",
    "df_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Relase</th>\n",
       "      <th>Director</th>\n",
       "      <th>Actors/Actresses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eterno resplandor de una mente sin recuerdos</td>\n",
       "      <td>(2004)</td>\n",
       "      <td>Michel Gondry (dir.)</td>\n",
       "      <td>Jim Carrey, Kate Winslet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El señor de los anillos: El retorno del rey</td>\n",
       "      <td>(2003)</td>\n",
       "      <td>Peter Jackson (dir.)</td>\n",
       "      <td>Elijah Wood, Viggo Mortensen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La vida de los otros</td>\n",
       "      <td>(2006)</td>\n",
       "      <td>Florian Henckel von Donnersmarck (dir.)</td>\n",
       "      <td>Ulrich Mühe, Martina Gedeck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harakiri</td>\n",
       "      <td>(1962)</td>\n",
       "      <td>Masaki Kobayashi (dir.)</td>\n",
       "      <td>Tatsuya Nakadai, Akira Ishihama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La tumba de las luciérnagas</td>\n",
       "      <td>(1988)</td>\n",
       "      <td>Isao Takahata (dir.)</td>\n",
       "      <td>Tsutomu Tatsumi, Ayano Shiraishi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lawrence de Arabia</td>\n",
       "      <td>(1962)</td>\n",
       "      <td>David Lean (dir.)</td>\n",
       "      <td>Peter O'Toole, Alec Guinness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Parásitos</td>\n",
       "      <td>(2019)</td>\n",
       "      <td>Bong Joon Ho (dir.)</td>\n",
       "      <td>Song Kang-ho, Lee Sun-kyun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Los mejores años de nuestras vidas</td>\n",
       "      <td>(1946)</td>\n",
       "      <td>William Wyler (dir.)</td>\n",
       "      <td>Myrna Loy, Dana Andrews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cinema Paradiso</td>\n",
       "      <td>(1988)</td>\n",
       "      <td>Giuseppe Tornatore (dir.)</td>\n",
       "      <td>Philippe Noiret, Enzo Cannavale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Duro de matar</td>\n",
       "      <td>(1988)</td>\n",
       "      <td>John McTiernan (dir.)</td>\n",
       "      <td>Bruce Willis, Alan Rickman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Name  Relase  \\\n",
       "0  Eterno resplandor de una mente sin recuerdos  (2004)   \n",
       "1   El señor de los anillos: El retorno del rey  (2003)   \n",
       "2                          La vida de los otros  (2006)   \n",
       "3                                      Harakiri  (1962)   \n",
       "4                   La tumba de las luciérnagas  (1988)   \n",
       "5                            Lawrence de Arabia  (1962)   \n",
       "6                                     Parásitos  (2019)   \n",
       "7            Los mejores años de nuestras vidas  (1946)   \n",
       "8                               Cinema Paradiso  (1988)   \n",
       "9                                 Duro de matar  (1988)   \n",
       "\n",
       "                                  Director                   Actors/Actresses  \n",
       "0                     Michel Gondry (dir.)           Jim Carrey, Kate Winslet  \n",
       "1                     Peter Jackson (dir.)       Elijah Wood, Viggo Mortensen  \n",
       "2  Florian Henckel von Donnersmarck (dir.)        Ulrich Mühe, Martina Gedeck  \n",
       "3                  Masaki Kobayashi (dir.)    Tatsuya Nakadai, Akira Ishihama  \n",
       "4                     Isao Takahata (dir.)   Tsutomu Tatsumi, Ayano Shiraishi  \n",
       "5                        David Lean (dir.)       Peter O'Toole, Alec Guinness  \n",
       "6                      Bong Joon Ho (dir.)         Song Kang-ho, Lee Sun-kyun  \n",
       "7                     William Wyler (dir.)            Myrna Loy, Dana Andrews  \n",
       "8                Giuseppe Tornatore (dir.)    Philippe Noiret, Enzo Cannavale  \n",
       "9                    John McTiernan (dir.)         Bruce Willis, Alan Rickman  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_random = []\n",
    "for i in range(10):\n",
    "    movies_random.append(random.choice(info_total))\n",
    "    \n",
    "df_movies_random = pd.DataFrame(movies_random, columns=['Name','Relase','Director','Actors/Actresses'])\n",
    "df_movies_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the city:merida\n"
     ]
    }
   ],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://api.openweathermap.org/data/2.5/weather?q=merida&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Requiem Red</td>\n",
       "      <td>£22.65</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Dirty Little Secrets of Getting Your Dream...</td>\n",
       "      <td>£33.34</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Coming Woman: A Novel Based on the Life of...</td>\n",
       "      <td>£17.93</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Boys in the Boat: Nine Americans and Their...</td>\n",
       "      <td>£22.60</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Black Maria</td>\n",
       "      <td>£52.15</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Starving Hearts (Triangular Trade Trilogy, #1)</td>\n",
       "      <td>£13.99</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shakespeare's Sonnets</td>\n",
       "      <td>£20.66</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Set Me Free</td>\n",
       "      <td>£17.46</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Scott Pilgrim's Precious Little Life (Scott Pi...</td>\n",
       "      <td>£52.29</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rip it Up and Start Again</td>\n",
       "      <td>£35.02</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Our Band Could Be Your Life: Scenes from the A...</td>\n",
       "      <td>£57.25</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Olio</td>\n",
       "      <td>£23.88</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mesaerion: The Best Science Fiction Stories 18...</td>\n",
       "      <td>£37.59</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Libertarianism for Beginners</td>\n",
       "      <td>£51.33</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>£45.17</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name   Price     Stock\n",
       "0                                A Light in the Attic  £51.77  In stock\n",
       "1                                  Tipping the Velvet  £53.74  In stock\n",
       "2                                          Soumission  £50.10  In stock\n",
       "3                                       Sharp Objects  £47.82  In stock\n",
       "4               Sapiens: A Brief History of Humankind  £54.23  In stock\n",
       "5                                     The Requiem Red  £22.65  In stock\n",
       "6   The Dirty Little Secrets of Getting Your Dream...  £33.34  In stock\n",
       "7   The Coming Woman: A Novel Based on the Life of...  £17.93  In stock\n",
       "8   The Boys in the Boat: Nine Americans and Their...  £22.60  In stock\n",
       "9                                     The Black Maria  £52.15  In stock\n",
       "10     Starving Hearts (Triangular Trade Trilogy, #1)  £13.99  In stock\n",
       "11                              Shakespeare's Sonnets  £20.66  In stock\n",
       "12                                        Set Me Free  £17.46  In stock\n",
       "13  Scott Pilgrim's Precious Little Life (Scott Pi...  £52.29  In stock\n",
       "14                          Rip it Up and Start Again  £35.02  In stock\n",
       "15  Our Band Could Be Your Life: Scenes from the A...  £57.25  In stock\n",
       "16                                               Olio  £23.88  In stock\n",
       "17  Mesaerion: The Best Science Fiction Stories 18...  £37.59  In stock\n",
       "18                       Libertarianism for Beginners  £51.33  In stock\n",
       "19                            It's Only the Himalayas  £45.17  In stock"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_doc = requests.get(url);\n",
    "\n",
    "soup = BeautifulSoup(url_doc.content, 'html.parser');\n",
    "test = soup.find_all('article', {'class':'product_pod'})\n",
    "titles = [(x.h3.extract()).a.extract()['title'] for x in test];\n",
    "prices = [(x.find('p', class_ = 'price_color')).get_text() for x in test];\n",
    "stock = [((x.find('p', class_ = 'instock availability')).get_text()).strip('\\n ') for x in test];\n",
    "#info = list(zip(titles, prices, stock))3info\n",
    "df_info  = pd.DataFrame(list(zip(titles, prices, stock)), columns = ['Name','Price','Stock'])\n",
    "df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
